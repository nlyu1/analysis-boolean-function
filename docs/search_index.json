[["index.html", "Algebraic Analysis of Classical and Quantum Computation Preface Notation", " Algebraic Analysis of Classical and Quantum Computation Nicholas Lyu 2024-06-02 Preface Reading notes and select exercise solutions for Ryan O’Donnell’s book Analysis of Boolean functions (O’Donnell 2014). In addition to being a rich field of computer science in its own right, the fourier analysis of boolean functions provide a classical basis for the fourier analysis of quantum computation. The space of boolean functions \\(f(x_1, \\cdots, x_n)\\) over \\(n\\) inputs form a freely generated algebra in which the generators \\(x_1, \\cdots x_n\\) are involutary, commutative, and generate an orthonormal basis. Subtly generalizing these constraints yield the full qubit algebra: Replacing commutativity over \\(x_1, \\cdots, x_{2n}\\) with anti-commutativity yields the majorana algebra over a finite number of modes, or equivalently the qubit algebra under the Jordan-wigner transform. Supplementing \\(x_1, \\cdots, x_{2n}\\) with \\(p_1, \\cdots p_{2n}\\) such that \\([x_i, p_j] = \\delta_{ij} x_ip_j\\) corresponds to the Pauli algebra with \\(x_i \\mapsto X_i, p_i\\mapsto Z_i\\). These notes are organized as follows Section 1 corresponds to Chapters 1 &amp; 8 of the book, introducing fourier concepts with boolean functions then generalizing to other domains. Section 2.1 expores the stability and dependence of boolean functions. Notation \\(\\mathcal P=\\{\\pm 1, \\times\\}\\) for parity, and \\(\\mathcal B=(\\{0, 1\\}, +)\\) for boolean values. \\([n] \\equiv \\{1, \\cdots, n\\}\\), occasionally \\(\\{0, \\cdots, n-1\\}\\) depending on context. \\(\\Delta(x, y)=|\\{i|x_i\\neq y_i\\}|\\) denotes the Hamming distance. \\(A\\Delta B\\) denote the symmetric difference \\((A-A\\cap B)\\cup (B-A\\cap B)\\). Boldface letters like \\(\\mathbf x\\) denote random variables, drawn from a uniform distribution over their domain unless otherwise specified. Under the mapping \\((0, 1)\\) corresponding to false, true, the representation of truth values in \\(\\mathcal P\\) is somewhat counter-intuitively \\((-1, 1)\\mapsto\\) true, false. Given a bitstring \\(x\\) and index \\(i\\), \\(x^{\\oplus i}\\) denotes \\(x\\) with the \\(i\\)-th index negated. Similarly, \\(x^{i\\mapsto b}\\) denotes \\(x\\) with the \\(i\\)-th index replaced by \\(b\\). Let \\(E\\) denote an event, for example, \\(a=b\\). Then \\(1_E\\) denotes the indicator random variable which is \\(1\\) when the event is true, and \\(0\\) otherwise. References O’Donnell, Ryan. 2014. Analysis of Boolean Functions. Cambridge University Press. "],["Sec1.html", "1 Fourier Expansion for General Domain 1.1 Boolean function", " 1 Fourier Expansion for General Domain This section corresponds to chapters \\(1\\) and \\(8\\). We demonstrating how the space of Boolean functions \\(\\mathcal P\\to \\mathbb R\\) form a Hilbert space, and then generalize this construction. 1.1 Boolean function Natural and parity bases Fix \\(n\\) and consider Boolean functions \\(\\mathcal P^n\\to \\mathcal P\\). Without loss of generality we may expand their range to \\(\\mathbb F\\in \\{\\mathbb R, \\mathbb C\\}\\) and define \\[ L^2(\\mathcal P^n) = \\{\\text{all functions $\\mathcal P^n\\to \\mathbb F$}\\} \\] Addition and scalar multiplication are defined in the most obvious way. Definition 1.1 (natural basis of boolean functions) Let \\(I_y\\) where \\(y\\in \\mathcal P^n\\) denote the indicator polynomial \\[ I_y(x) = \\begin{cases} 1 &amp; x = y \\\\ 0 &amp; \\text{else} \\end{cases} \\] The indicator polynomials form the natural basis for boolean functions, since for all \\(f\\in L^2(\\mathcal P^n)\\) \\[ f(x) = f(y)I_y(x) \\implies f = f(y)I_y \\] Note the implicit sum over \\(y\\in \\mathcal P^n\\). Let \\(x_i(y) = y_i, i\\in [n]\\); this is overloading the notation, as \\(x_i\\) can now both be a function or an input variable, depending on context. Definition 1.2 (parity function) Given \\(J\\in \\mathcal P^n\\), define the parity function \\(\\chi_J:\\mathcal P^n\\to \\mathbb F\\). \\[ \\chi_J = \\prod_{J_i=1} x_i \\implies \\chi_J(x) = (-1)^{J\\cdot x} \\] Every indicator function can be expanded in parity functions \\[\\begin{equation} I_y = \\prod_{i=1}^n \\dfrac{1+y_ix_i}{2} \\impliedby (\\exists i: x_i\\neq y_i\\implies I_y=0) \\tag{1.1} \\end{equation}\\] As a consequence, every indicator function \\(I_y\\), thus every boolean function, can be expanded as a multi-linear polynomial in \\((x_j)\\). Note that multilinearity is due to the multilinearity of the indicator expansion (1.1) and the fact that in the \\(\\mathbb F_2\\) representation, \\(x_i^2=1\\). Fourier formulas Theorem 1.1 (fourier expansion of boolean functions) Every boolean function \\(f\\in L^2(\\mathcal P^n)\\) is a unique linear combination of parity functions \\[ f = \\widehat f(J) \\chi_J \\] The coefficients \\(\\widehat f(J)\\) form the fourier spectrum of \\(f\\). Equivalently, \\(\\widehat f\\in L^2(\\mathbb F^n)\\) is the fourier transform of \\(f\\). Note that the range of \\(\\widehat f\\) is in general not \\(\\mathcal P\\). Proof: Note that \\(\\dim L^2(\\mathcal P^n) = 2^n\\). The natural basis \\((I_J)\\) and parity basis \\((\\chi_J)\\) both contain \\(2^n\\) elements and there is a linear transformation between them, so both are bases for \\(L^2(\\mathcal P^n)\\). Remark (product structure of fourier basis). Note that the parity basis function \\(\\chi_J\\) is a product of elements which are supported on (dependent upon) only a component of the input. This is not true of the natural basis. Definition 1.3 (natural and fourier expansions of max) Consider \\(\\max_2\\) with \\(\\max_2(x, y)=-1\\) if \\(x=y=-1\\) else \\(1\\). Using the formula above \\[\\begin{align} {\\max}_2 &amp;= I_{(1, 1)} + I_{(1, -1)} + I_{(-1, 1)} - I_{(-1, -1)} \\\\ &amp;= \\dfrac 1 4 (1+x_1)(1+x_2) + \\dfrac 1 4 (1+x_1)(1-x_2) + \\dfrac 1 4 (1-x_1)(1+x_2) - \\dfrac 1 4 (1-x_1)(1-x_2) \\\\ &amp;= \\cdots = \\dfrac 1 2 (1 + x_1 + x_2 - x_1x_2) \\end{align}\\] Definition 1.4 (inner product) Define the inner product \\(\\langle\\cdot, \\cdot\\rangle: L^2(\\mathcal P^n)\\times L^2(\\mathcal P^n)\\to \\mathbb F\\) via \\[ \\langle f, g\\rangle= \\dfrac 1 {2^n} \\langle f(x), g(x)\\rangle \\] Again, note the implicit sum over \\(x\\). Here \\(\\langle f(x), g(x)\\rangle\\) is the inner product on \\(\\mathbb F\\) \\[ \\langle a, b\\rangle= \\begin{cases} ab &amp; \\mathbb F=\\mathbb R\\\\ a^*b &amp; \\mathbb F=\\mathbb C \\end{cases} \\] In most generality \\(\\mathbb F=\\mathbb C\\), in which case \\(\\langle f, g\\rangle= \\mathbb E[f(\\mathbf x)^*g(\\mathbf x)]\\). Norms are defined naturally \\[ \\|f\\|_p = \\mathbb E[|f(\\mathbf x)|^{p}]^{1/p} \\] The indicator basis is apparantly orthonormal since \\(I_xI_y=0\\) if \\(x\\neq y\\). Theorem 1.2 (orthonormality of fourier basis) \\(\\langle\\chi_J, \\chi_K\\rangle= \\delta_{JK}\\). Proof: Using \\(x_j^2=1\\), we obtain \\(\\chi_J\\chi_K=\\chi_{J\\Delta K}\\). We further have \\(\\mathbb E[\\chi_J(\\mathbf x)] = \\delta_{J 0}\\), then \\[ \\langle\\chi_J, \\chi_K\\rangle= \\mathbb E[\\chi_J(\\mathbf x)\\chi_K(\\mathbf x)] = \\mathbb E[\\chi_{J\\Delta K}(\\mathbf x)] = \\delta_{0(J\\Delta K)} = \\delta_{JK} \\] One may also verify that this is non-degenerate, bilinear and (skew)-symmetric. The usual projection and norm formulas follow. Theorem 1.3 (Parseval's theorem) \\(\\|f\\|_2^2 = \\mathbb E[|f(\\mathbf x)|^2] = \\sum_J |\\widehat f(J)|^2\\) In particular, if \\(f\\) is boolean-valued (range in \\(\\mathbb F_2\\)) then \\(\\langle f|f\\rangle= 1\\). Theorem 1.4 (Plancherel's theorem) \\(\\langle f, g\\rangle= \\mathbb E[f(\\mathbf x)^*g(\\mathbf x)] = \\widehat f(J)^*g(J)\\) Definition 1.5 (Hamming distance) Define the Hamming distance between two boolean functions with range \\(\\mathcal P\\) \\[ \\mathrm{dist}(f, g) = \\Pr[f(\\mathbf x)\\neq g(\\mathbf x)] \\] Note that \\(\\langle f, g\\rangle= 1 - 2\\, \\mathrm{dist}(f, g)\\). Definition 1.6 (boolean function statistics) A boolean function, viewed as \\(f(\\mathbf x)\\), is also a real-valued discrete random variable. Its statistics are defined in the most obvious ways \\(\\mathbb E[f] = \\widehat f(0)\\); here we are using \\(\\mathcal B\\) and \\(\\mathcal P\\) interchangeably. This is the constant term in the fourier expansion of \\(f\\) about \\(\\mathcal B\\) representation. \\(\\mathrm{Cov}(f, g) = \\langle f , g\\rangle- \\mathbb E[f]\\mathbb E[g] = \\sum_{J\\neq 0} \\widehat f(J) g(J)\\). \\(\\mathrm{Var}(f) = \\|f-\\mathbb E[f]\\|^2 = \\sum_{j\\neq 0} f(J)^2\\). Definition 1.7 (fourier weight, spectral sample, degree weight) The (fourier) weight of \\(f\\) on a set \\(J\\) is simply \\(|\\widehat f(J)|^2\\). In particular, Parseval’s theorem implies that the fourier weight of a boolean function defines a probability distribution on the Hamming cube, which may be identified with the power set \\(\\mathcal P([n])\\). The spectral sample \\(\\mathbf S_f\\) is the probability distribution over \\(\\mathcal P([n])\\) according to \\(\\widehat f\\). Each multi-index \\(J\\) has a degree (its polynomial degree in \\((x_j)\\)). The fourier weight of \\(f\\) at degree \\(k\\) is \\[ \\mathbf W^k[f] = \\sum_{|J|=k} |\\widehat f(J)|^2 = \\Pr_{S\\sim \\mathcal S_f}[|S|=k] \\] Relative densities and convolution For this section we use \\(\\mathcal B\\) instead of \\(\\mathcal P\\) to represent the boolean domain, so that addition is the natural operation. Definition 1.8 (density function) A (relative probability) density function on the Hamming cube \\(\\mathcal P^n\\) is a nonnegative function satisfying \\(\\mathbb E[\\varphi(\\mathbf x)] = 1\\). In this case we interchangeably write \\(\\varphi\\) as a density function itself. Given density \\(\\varphi\\), note that \\[ \\mathbb E_{\\mathbf y\\sim \\varphi}[g(\\mathbf y)] = \\dfrac 1 {2^n} \\varphi(y)g(y) = \\langle\\varphi, g\\rangle \\] Definition 1.9 (indicator function) Given \\(A\\subset \\mathcal B^n\\), let \\(1_A:\\mathcal B^n\\to \\mathcal B\\) for the \\((0, 1)\\) indicator function. For \\(A\\neq \\emptyset\\), denote by \\(\\varphi_A = 1_A/\\mathbb E[1_A]\\) the density function associated with uniform distribution over \\(A\\). Example 1.1 (singleton density) Let \\(\\{0\\}\\subset \\mathcal B^n\\) denote the singleton set of the vector with all entries \\(0\\) \\[ \\varphi_{\\{0\\}}(y) = 2^n I_{\\{0\\}}(y) = 2^n \\prod (1+y_j) = \\sum_J \\chi_J(y) \\] Note that \\(\\pm\\) are equivalent in the \\(\\mathcal B\\) algebra. Definition 1.10 (convolution) Given \\(f, g:\\mathcal B^n\\to \\mathbb R\\), define \\(f\\ast g:\\mathcal P^n\\to \\mathbb R\\) by \\[\\begin{equation} (f\\ast g)(x) = \\mathbb E[f(\\mathbf y)g(x\\pm \\mathbf y)] \\tag{1.2} \\end{equation}\\] The counterpart for \\(f, g\\in \\mathcal P^n\\to \\mathbb R\\) is \\((f\\ast g)(x) = \\mathbb E[f(\\mathbf y)g(x\\mathbf y)]\\). Note that if \\(\\varphi\\) is a density function, then \\[ (\\varphi \\ast g)(x) = \\mathbb E_{y\\sim \\varphi}[g(x+\\mathbf y)] \\implies \\mathbb E_{\\mathbf y\\sim \\varphi}[g(\\mathbf y)] = (\\varphi \\ast g)(0) \\] Theorem 1.5 (fourier theorem) Given \\(f, g:\\mathcal B^n\\to \\mathbb C\\), for all \\(J\\) \\[ \\widehat{f\\ast g}(J) = \\widehat f(J) \\widehat g(J) \\] Proof: Using the projection formula, definition, substitute \\(\\mathbf z=\\mathbf x - \\mathbf y\\), and \\(\\chi_J(x+y)=J\\cdot (x+y) = \\chi_J(x)\\chi_J(y)\\) \\[\\begin{align} \\widehat{f\\ast g}(J) &amp;= \\mathbb E_{\\mathbf x}[(f\\ast g)(\\mathbf x) \\chi_J(\\mathbf x)] = \\mathbb E_{\\mathbf{x, y}}[f(\\mathbf y)g(\\mathbf x - \\mathbf y) \\chi_J(\\mathbf x)] \\\\ &amp;= \\mathbb E_{\\mathbf{x, z}}[f(\\mathbf y)g(\\mathbf z)\\chi_J(\\mathbf z + \\mathbf y)] \\\\ &amp;= \\mathbb E_{\\mathbf{x, z}}[f(\\mathbf y)\\chi_J(\\mathbf y) f(\\mathbf z)\\chi_J(\\mathbf z)] = \\widehat f(J)\\widehat g(J) \\end{align}\\] This suffices to prove that convolution is associative and commutative. Almost linear functions, property testing, and BLR Definition 1.11 ((approximate) linearity of boolean functions) A function \\(f:\\mathcal B^n\\to \\mathcal B\\) is linear if either of the following conditions hold: \\(\\forall x, y: f(x+y) = f(x)+f(y)\\) \\(f(x) = a\\cdot x\\) for some \\(a\\in \\mathcal B^n\\) These conditions are indeed equivalent: \\(2\\implies 1\\) is trivial. Given \\(1\\), define \\((e_j)_k=\\delta_{jk}\\), then \\((2)\\) is satisfied by \\(a_j=f(e_j)\\). We propose two definitions of approximately linear: \\(f(x+y)=f(x)+f(y)\\) for almost all pairs \\(x, y\\) For some \\(a\\), \\(f(x)=a\\cdot x\\) for almost all \\(x\\). Here \\(2\\implies 1\\) is robust: if \\((2)\\) is satisfied for \\(x, y\\), then \\(1\\) is satisfied for the pair, too. The converse is not papparant but nevertheless true. Definition 1.12 (closeness) Recalling the Hamming distance 1.5. Two boolean functions are \\(\\epsilon\\)-close of \\(\\mathrm{dist}(f, g)\\leq \\epsilon\\) otherwise \\(\\epsilon\\)-far. Given a nonempty property \\(P\\) on \\(n\\)-bit Boolean functions, define \\[ \\mathrm{dist}(f, P) = \\min_{g\\in P} \\mathrm{dist}(f, g) \\] We define approximate linearity by criterion (2) above: \\(f\\) is \\(\\epsilon\\)-close to being linear if there is a linear function \\(x\\mapsto a\\cdot x\\) which is less than \\(\\epsilon\\)-close to \\(f\\). The test below shows that \\(1\\implies 2\\) above: Theorem 1.6 (BLR test) The BLR procedure tests for linearity as follows: Choose \\(\\mathbf{x, y}\\in \\mathcal B^n\\) independently. Query \\(f\\) at \\(\\mathbf{x, y}, \\mathbf x + \\mathbf y\\). Accept if \\(f(\\mathbf x)+f(\\mathbf y) = f(\\mathbf x + \\mathbf y)\\). If BLR test accepts \\(f\\) with probability \\(1-\\epsilon\\), then \\(f\\) is \\(\\epsilon\\)-close to being linear. Proof: Encode output by \\(\\pm 1\\) so that acceptance condition becomes \\(f(\\mathbf x)f(\\mathbf y) = f(\\mathbf x + \\mathbf y)\\), then \\[ \\dfrac 1 2 + \\dfrac 1 2 f(\\mathbf x)f(\\mathbf y)f(\\mathbf x + \\mathbf y) = \\begin{cases} 1 &amp; f(\\mathbf x)f(\\mathbf y) = f(\\mathbf x + \\mathbf y) \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Note that the input representation is still \\(\\mathcal B\\) so formula (1.2) still applies, then \\[\\begin{align} 1-\\epsilon &amp;= \\Pr(\\text{accept}) = \\dfrac 1 2 + \\dfrac 1 2 \\mathbb E_{\\mathbf x}[f(\\mathbf x) \\mathbb E_{\\mathbf y}[f(\\mathbf y)f(\\mathbf x + \\mathbf y)]] \\\\ &amp;= \\dfrac 1 2 + \\dfrac 1 2 \\mathbb E_{\\mathbf x}[f(\\mathbf x) (f\\ast f)(\\mathbf x)] \\\\ &amp;= \\dfrac 1 2 + \\dfrac 1 2 \\langle f, f\\ast f\\rangle= \\dfrac 1 2 + \\dfrac 1 2 \\sum_J \\hat f(J)^3 \\\\ 1 - 2\\epsilon &amp;= \\sum_J \\hat f(J)^3 \\leq [\\max_K \\hat f(K)] \\sum_J \\hat f(J)^2 = \\max_K \\hat f(K) \\end{align}\\] For the last equation we used \\(\\sum \\hat f(J)^2 = \\langle f, f\\rangle= \\mathbb E[f^2] = 1\\). Now, recall that all linear functions are of the form \\(\\chi_J\\), then \\[ 1 - 2\\epsilon \\max_K \\hat f(K) = \\max_K \\langle f, \\chi_K\\rangle= \\max_K [1 - 2\\mathrm{dist}(f, \\chi_K)] = 1 - 2\\min_K \\mathrm{dist}(f, \\chi_K) \\] It follows that \\(\\epsilon \\geq \\min_K \\mathrm{dist}(f, \\chi_K)\\). If \\(f\\) is \\(\\epsilon\\)-close to \\(\\chi_J\\), we can determine \\(\\chi_J(x)\\) with high probability for a given \\(x\\) using the following procedure: Proposition 1.1 (local correctibility of linear functions) Suppose \\(f:\\mathcal B^n\\to \\mathcal P\\) is \\(\\epsilon\\)-close to \\(\\chi_J\\), then for every \\(x\\in \\mathcal B^n\\), the following procedure outputs \\(\\chi_J(x)\\) with probability at least \\(1-2\\epsilon\\): Choose \\(\\mathbf y\\sim \\mathcal B^n\\) Query \\(f\\) at \\(\\mathbf y\\) and \\(x+\\mathbf y\\) Output \\(f(\\mathbf y)f(x+\\mathbf y)\\) Note that even if \\(f(x)\\neq \\chi_J(x)\\), the procedure above is still able to determine \\(\\chi_J(x)\\) with high probability based on \\(f\\). Proof: The probability that \\(f(x+\\mathbf y)\\neq \\chi_J(x+\\mathbf y)\\) and \\(f(\\mathbf y)\\neq \\chi_J(\\mathbf y)\\) is upper-bounded by \\(1-2\\epsilon\\). "],["influence-and-spectral-learning.html", "2 Influence and Spectral Learning 2.1 Influence, Noise Stablity, and Social Choice", " 2 Influence and Spectral Learning 2.1 Influence, Noise Stablity, and Social Choice This subsection corresponds to Chapter 2. Our focus is on the uniformity and dependence behavior of functions on their inputs. Social choice functions A boolean function \\(\\mathcal P^n\\to \\mathcal P\\) may also be viewed as a voting function for an election with \\(2\\) candidates and \\(n\\) voters. The most common voting function, for odd \\(n\\) is \\[ \\mathrm{Maj}_n(x) = \\mathrm{sgn}\\left(\\sum x_j\\right) \\] Definition 2.1 (dictator, junta) The \\(i\\)-th dictator (projection) function is \\(\\chi_i(x) = x_i\\). Similarly, a function \\(\\mathcal P^n\\to \\mathcal P\\) is a \\(k\\)-junta if it depends upon at most \\(k\\) input coordinates. Informally, a function is a junta if it depends on a constant number of coordinates. Definition 2.2 (weighted majority (linear threshold) function) \\(f\\) is a weighted majority function or, equivalently, a (linear) threshold function if, for some \\(a\\in \\mathbb R^n\\) \\[ f(x) = \\mathrm{sgn}(a\\cdot x) \\] A depth-\\(d\\) recursive majority of \\(n\\) functions, denoted \\(\\mathrm{Maj}_n^{\\otimes d}\\) is a function of \\(n^d\\) bits defined inductively \\[ \\mathrm{Maj}_n^{\\otimes (d+1)}(x^{(1)}, \\cdots, x^{(n)}) = \\mathrm{Maj}_n( \\mathrm{Maj}_n^{\\otimes d}(x^{(1)}), \\cdots, \\mathrm{Maj}_n^{\\otimes d}(x^{(n)}) ) \\] Definition 2.3 (tribes function) The tribes function of width \\(w\\) and size \\(s\\) \\(\\mathrm{Tribes}_{ws}:\\mathcal P^{ws}\\to \\mathcal P\\) is \\[ \\mathrm{Tribes}((x_{ws})) = \\bigcup_{j=1}^s \\left(\\bigcap_{k=1}^w x_{ws}\\right) \\] There are \\(s\\) tribes, each containing \\(w\\) elements. Dependence properties Definition 2.4 (miscellaneous properties) \\(f:\\mathcal P^n\\to \\mathcal P\\) (naturally extending to codomain \\(\\mathbb R\\)) is monotone if \\(x\\leq y\\implies f(x)\\leq f(y)\\) where \\(x\\leq y\\) is defined coordinate-wise. odd if \\(f(-x) = -f(x)\\). unanimous if \\(f(1, \\cdots, 1)=1\\) and \\(f(-1, \\cdots, -1)=-1\\). symmetric if \\(f\\circ \\pi = f\\) for all permutations. In other words, \\(f\\) only depends on the weight of \\(x\\). Theorem 2.1 (May's theorem) \\(f:\\mathcal P^n \\to \\mathcal P\\) is symmetric and monotone if and only if \\(f(x)=\\mathrm{sgn}(1\\cdot x+a_0)\\). If \\(f\\) is additionally odd, then \\(n\\) must be odd and \\(f=\\mathrm{Maj}_n\\). Proof: Symmetry demands \\(f(x)=g(1\\cdot x)\\) for some \\(g:\\mathbb N\\to \\mathbb R\\). One may show that \\(y\\mapsto \\mathrm{sgn}(|y|+a_0)\\) is the only admissible choice. Properties of functions we have seen so far: Majority function for \\(n\\) odd has all four properties. The dictator function is monotone, odd, and unanimous, so do recursive majority functions. Tribes functions are monotone, unanimous, and transitive-symmetric. Definition 2.5 (transitive symmetry) \\(f\\) is transitive-symmetric if for all \\(j, k\\in [n]\\) there exists a permutation \\(\\pi\\in S_n\\) with \\(\\pi(j)=k\\) such that \\(f\\circ \\pi = f\\). Symmetry demands that all arrangements of coordinates are equivalent. Transitive symmetry only demands that every pair of coordinates are equivalent. Proposition 2.1 If \\(f\\) is transitive symmetric, then \\[ \\forall j, k: \\hat f(j) = \\hat f(k) \\] Proof: If \\(f=f\\circ \\pi\\), where \\(\\pi\\) is a permutation, then \\(\\hat f = \\hat f\\circ \\pi^{-1}\\) since \\(\\chi_J\\circ \\pi = \\chi_{\\pi^{-1}(J)}\\). The result follows by definition of transitive symmetry. Definition 2.6 (impartial culture assumption) The impartial culture assumption assumes that all \\(n\\) voters’ preferences are independent and uniformly random. This corresponds to voting in the absence of information. Influence, derivative, and expectation Given a coordinate \\(i\\in [n]\\) and binary string \\(x\\), let \\(x^{\\oplus i}\\) denote the same string except with the \\(i\\)-th component flipped. Definition 2.7 (pivot, influence) A coordinate \\(i\\in [n]\\) is pivotal for \\(f\\) on input \\(x\\) if \\(f(x)\\neq f(x^{\\oplus i})\\). The influence of \\(i\\) on \\(f\\) is the probability that \\(i\\) is pivotal for random input: \\[ \\mathrm{Inf}_i[f] = \\Pr[f(\\mathbf x)\\neq f(\\mathbf x^{\\oplus i})] \\] One may consider a boolean function providing a \\(2\\)-coloring of the hamming cube. Each boundary edge which connect vertices of different colorings then correspond a “change in decision.” Consequently \\(\\mathrm{Inf}_i\\) is the fraction of boundary edges out of edges which involve a change in \\(x_i\\). The dictator function satisfies \\(\\mathrm{Inf}_j[\\pm \\chi_k]=\\delta_{jk}\\). The “or” and “and” functions satisfy \\(\\mathrm{Inf}_i[\\mathrm{OR}_n] = \\mathrm{Inf}_i[\\mathrm{AND}_n] = 2^{1-n}\\). Definition 2.8 (derivative, gradient) The \\(i\\)-th (discrete) derivative operator \\(\\partial_{i}:L^2(\\mathcal P^n)\\to L^2(\\mathcal P^n)\\) is defined by \\[ \\partial_{i} f(x) = \\dfrac{f(x^{i\\mapsto 1}) - f(x^{i\\mapsto 0})} 2 \\] Note that \\(\\partial_{i} f\\) does not depend on \\(x_i\\) anymore. It is also manifestly linear and obeys the product rule for the boolean function algebra. \\[ \\partial_{i} fg = (\\partial_{i} f)g + f(\\partial_{i} g) \\] Note that this is only true for \\(f, g\\in L^2(\\mathcal P^n)\\), which are multilinear polynomials in \\((\\chi_i)\\). Then \\[ \\partial_{i} \\chi_J = \\begin{cases} \\chi_{J-\\{i\\}} &amp; i\\in S \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\implies \\partial_{i} f = \\sum_{i\\in J} \\hat f(J) \\chi_{J - \\{i\\}} \\] We also have a fourier formula for influence \\[\\begin{equation} \\mathrm{Inf}_i[f] = \\mathbb E[(\\partial_{i} f(\\mathbf x))^2] = \\|\\partial_{i} f\\|_2^2 = \\sum_{i\\in J} \\hat f(J)^2 \\tag{2.1} \\end{equation}\\] The influence of coordinate \\(i\\) on \\(f\\) is the sum of the fourier weights of \\(f\\) on sets containing \\(i\\). This constitutes the generalization of influence to real-valued boolean functions. Definition 2.9 (relevance) Coordinate \\(i\\) is relevant for \\(f\\) if \\(\\mathrm{Inf}_i[f]&gt;0\\). In other words, there exists at least one \\(x\\) for which \\(f(x)\\neq f(x^{\\oplus i})\\). Proposition 2.2 (influence of monotone functions) If \\(f\\) is monotone, then \\(\\mathrm{Inf}_i[f] = \\hat f(i)\\). Proof: By monoticity, \\(\\partial_{i} f(x) = 1\\) (and never \\(-1\\)) if coordinate \\(i\\) is pivotal for \\(x\\). Then \\[ \\mathrm{Inf}_i[f] = \\mathbb E[\\partial_{i} f] = \\langle\\partial_{i} f, \\chi_{\\emptyset}\\rangle= \\widehat{\\partial_{i} f}(0) = \\hat f(i) \\] Proposition 2.3 If \\(f\\) is transitive-symmetric and monotone, then \\[ \\forall i: \\inf_i[f] \\leq 1/\\sqrt n \\] Proof: the intuition here is that influence can only be positive, the total influence from all positions are bounded, and that any two positions are roughly equivalent. By proposition 2.1 \\(\\mathrm{Inf}_i[f] = \\hat f(1)\\) independent of \\(i\\). By Parseval \\[ \\mathbb E[f^2] = 1 = \\sum \\hat f(J)^2 \\geq \\sum_j \\hat f(j)^2 = n\\hat f(1)^2 \\implies \\mathrm{Inf}_i[f] = \\hat f(1) \\leq 1/\\sqrt n \\] Definition 2.10 (expectation operator, Laplacian) The \\(i\\)-th expectation operator is the linear operator \\[ \\mathbb E_i f(x) = \\mathbb E_{\\mathbf x_i}[f(x_1, \\cdots, \\mathbf x_i, \\cdots, x_n)] \\] Note that \\(\\mathbb E_i f\\) again does not depend on \\(x_i\\). For boolean functions, following the analysis for \\(\\partial_{j}\\) above \\[\\begin{align} \\mathbb E_i f(x) &amp;= \\dfrac{f(x^{i\\to 1}) + f(x^{(i\\to -1)})} 2 \\\\ f(x) &amp;= x_j \\partial_{j} f(x) + \\mathbb E_j f(x) \\quad \\text{for any fixed $j$} \\\\ \\mathbb E_i f &amp;= \\sum_{i\\notin J} \\hat f(J) \\chi_J \\end{align}\\] The coordinate Laplacian operator \\(\\mathrm{L}_i\\) is defined by \\[ \\mathrm{L}_i f = f - \\mathbb E_i f = x_i \\partial_{i} f \\] It follows immediately from fourier formulas and previous definitions that \\[\\begin{align} \\mathrm{L}_i f(x) &amp;= x_i \\partial_{i} f(x) = \\dfrac{f(x) - f(x^{\\oplus i})} 2 = \\sum_{i\\in J}\\hat f(J) \\chi_J\\\\ \\mathrm{Inf}_i[f] &amp;= \\sum_{i\\in J} \\hat f(J)^2 = \\langle\\mathrm{L}_i f, \\mathrm{L}_i f\\rangle= \\langle f, \\mathrm{L}_i f\\rangle \\tag{2.2} \\end{align}\\] Total influence Definition 2.11 (total influence, sensitivity) The total influence of \\(f:\\mathcal P^n\\to \\mathbb R\\) is \\[ \\mathrm{Inf}_\\Sigma[f] = \\sum \\mathrm{Inf}_j[f] \\] The sensitivity is defined as \\[ \\mathrm{sens}_f(x) = \\text{number of pivotal coordinates of $f$ at $x$} \\] For boolean functions, the total influence equal to the average sensitivity \\[ \\mathrm{Inf}_\\Sigma[f] = \\sum_j \\Pr[f(\\mathbf x)\\neq f(\\mathbf x^{\\oplus j}) = \\mathbb E\\left[\\sum_j 1_{f(\\mathbf x)\\neq f(\\mathbf x^{\\oplus j})}\\right] = \\mathbb E[\\mathrm{sens}_f(\\mathbf x)] \\] Proposition 2.2 implies that for monotone functions, \\(\\mathrm{Inf}_\\Sigma[f] = \\sum \\hat f(j)\\). Proposition 2.4 The fraction of boundary edges in the Hamming cube is \\(\\mathrm{Inf}_\\Sigma[f]/n\\). Proof: Follows immediately from our discussion 2.7 that \\(\\mathrm{Inf}_i[f]\\) is the fraction of boundary edges for \\(i\\)-component changing edges in the \\(f\\)-coloring of the Hamming cube. The linear fourier coefficients have a natural interpretation from a social choice perspective. Proposition 2.5 Let \\(f:\\mathcal P^n\\to \\mathcal P\\) be a voting rule for a \\(2\\)-candidate election. Given votes \\(\\mathbf x\\) and let \\(\\mathbf w\\) be the number of votes which agree with the outcome \\(f(\\mathbf x)\\) of the election, then \\[ \\mathbb E[\\mathbf w] = \\dfrac n 2 + \\dfrac 1 2 \\sum \\hat f(j) \\] Proof: Follows immediately from the definition of \\(\\mathbf w\\) and \\[ \\sum \\hat f(j) = \\mathbb E[f(\\mathbf x)(\\mathbf x_1 + \\cdots + \\mathbf x_n)] \\] Our discussion of component influence led to derivatives and Laplacians. Total influence correspondly prompt the following definitions The gradient \\(\\nabla f = (\\partial_{1} f, \\cdots, \\partial_{n} f) \\in L^2(\\mathcal P^n)^n\\). The Laplacian \\(\\mathrm{L}= \\sum \\mathrm{L}_j\\). The following fourier formulas for the Laplacian hold, by direct extension \\(\\mathrm{L}f(x) = \\dfrac n 2 \\left[f(x) - \\mathbb E_{i\\in [n]} f(x^{\\oplus i}) \\right]\\). For boolean-valued \\(f\\), \\(\\mathrm{L}f(x) = f(x)\\mathrm{sens}_f(x)\\). By linear extension of the previous result: \\[ \\mathrm{L}f = \\sum_J |J| \\hat f(J) \\chi_J \\] \\(\\langle f, \\mathrm{L}f\\rangle= \\mathrm{Inf}_\\Sigma[f]\\). Theorem 2.2 (spectral representation of total influence) The total influence is the expectation of fourier degree drawn according to the spectral sample \\[ \\mathrm{Inf}_\\Sigma[f] = \\sum_J |J| \\hat f(J)^2 = \\sum_j j W^j[f] \\implies \\mathrm{Inf}_\\Sigma[f] = \\mathbb E_{J\\sim \\mathcal S_f}[|\\mathbf J| \\] It measures the average “degree” of the fourier weights. Proof Recalling equation (2.2), when we sum over \\(i\\), each \\(J\\) with degree \\(|J|\\) gets counted exactly \\(j\\) times. Corollary 2.1 (Poincaré inequality) for any \\(f\\in L^2(\\mathcal P^n)\\) \\[ \\mathrm{Var}[f] = \\sum_{j&gt;0} W^j[f] \\leq \\sum_j jW^j[f] = \\mathrm{Inf}_\\Sigma[f] \\] Equality holding if and only if \\(f\\) is constant or a dictator. Proof: Equality holds if and only if the fourier weight of \\(f\\) is concentrated at \\(j=0\\) or \\(j=1\\). "],["quantum-analogues-of-boolean-analysis.html", "3 Quantum Analogues of Boolean Analysis 3.1 Boolean function algebra 3.2 Pauli algebra 3.3 Majorana algebra", " 3 Quantum Analogues of Boolean Analysis 3.1 Boolean function algebra Binary domain In addition to O’Donnell’s formalism in sections 1 and 2.1, we see that the space of boolean functions \\(L^2(\\mathcal P^n)\\) is in fact a finite freely-generated Hilbert algebra with \\(n\\) generators \\(x_j\\in L^2(\\mathcal P^n)\\) (with some convenient notation overloading) \\[ x_i(x) = x_i, \\quad x_i:\\mathcal P^n\\to \\mathbb C \\] Scalar multiplication and addition are defined in the most obvious ways, and algebraic multiplication given by \\[\\begin{equation} [x_i, x_j] = 0, \\quad x_i^2 = 1 \\tag{3.1} \\end{equation}\\] Given an algebraic element \\(f\\), its behavior as a boolean function is retrievable by \\[\\begin{equation} f(x) = 2^n\\langle f, I_x\\rangle= \\sum_y f(y) \\prod \\dfrac{1+x_jy_j}{2} \\tag{3.2} \\end{equation}\\] Note that the crucial \\(x_i^2=1\\) arises from the algebraic structure of \\(\\mathcal P^n\\). The generators generate the bases \\((\\chi_J)\\) of \\(L^2(\\mathcal P^n)\\) with \\(2^n\\) elements orthonormal w.r.t. the inner product \\[ \\langle f, g\\rangle= \\mathbb E_{\\mathbf x\\sim \\mathcal P^n} [f(\\mathbf x)^* g(\\mathbf x)] \\] There exists derivative operator defined by its generator action, Leibniz rule, and linearity \\[ \\partial_{i} \\chi_j = \\delta_{ij}, \\quad \\partial_{i} (fg) = (\\partial_{i} f)g + f(\\partial_{i} g) \\] General domain 3.2 Pauli algebra Our starting point is recognizing that equation (3.1) is exactly the commutation relation of the Pauli basis \\((X_j)\\) or \\((Z_j)\\). We argue that \\((Z_j)\\) is the quantum counterpart directly associated with the computational basis. Define the “classical” boolean function \\(\\tilde \\rho:\\mathcal P^n\\to \\mathbb R\\) associated with a quantum state \\(\\rho\\) to be the probability of observing \\(|x\\rangle\\) (note that we are using the \\(\\mathcal P=\\pm 1\\) labeling of the computational basis), then \\[ \\tilde \\rho(x) = \\Pr(x|\\rho) = \\langle x|\\rho|x\\rangle= \\mathrm{tr}\\left(\\rho \\prod \\dfrac{1+x_j Z_j}{2}\\right) \\] Comparing this with the classical counterpart (3.2) suggests that \\(x_j\\mapsto Z_j\\). Definition 3.1 (Pauli quantization of boolean function) Given a classical boolean function \\(f\\), its Pauli quantization is the diagonal quantum state \\(\\rho_f\\in \\mathcal H_n\\) specified by \\[ \\langle x|\\rho|x\\rangle= f(x) \\] This results in the following embedding of boolean function algebra into the Pauli algebra \\(\\mathcal P_n\\) \\[ x_j \\mapsto Z_j, \\quad \\langle f, g\\rangle= \\mathbb E_{\\mathbf x}[f(\\mathbf x)g(\\mathbf x)] \\mapsto \\langle\\rho_f, \\rho_g\\rangle= \\dfrac 1 {2^n} \\mathrm{tr}(\\rho_f\\rho_g) \\] The full quantum algebra \\(\\mathcal P_n\\) is generated by introducing \\(n\\) more involutary generators \\((X_j)\\) such that \\[ [Z_j, X_k] = 2\\delta_{jk}Z_jX_j, \\quad [X_j, X_k] = 0, \\quad X_j^2 = 1 \\] 3.3 Majorana algebra Another perspective on the similarity between quantum and classical computation is replacing the commutation relation in (3.1) with anti-commutation. However, there appears to be no canonical way of doing so. Definition 3.2 (Majorana algebra) The Majorana algebra is the Clifford algebra \\(\\mathcal C_{2n}\\) over \\(2n\\) generators such that distinct generators anti-commute \\[ \\{\\gamma_j, \\gamma_k\\} = \\delta_{jk}, \\quad \\gamma_j^2 = 1 \\] "],["references.html", "References", " References O’Donnell, Ryan. 2014. Analysis of Boolean Functions. Cambridge University Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
