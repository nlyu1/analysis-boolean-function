[["index.html", "Analysis of Boolean Functions Preface Notation", " Analysis of Boolean Functions Nicholas Lyu 2024-06-02 Preface Reading notes and select exercise solutions for Ryan O’Donnell’s book Analysis of Boolean functions (O’Donnell 2014). In addition to being a rich field of computer science in its own right, the fourier analysis of boolean functions provide a classical basis for the fourier analysis of qubit states. These notes are organized as follows Section 1 corresponds to Chapters 1 &amp; 8 of the book, introducing fourier concepts with boolean functions then generalizing to other domains. Section 2.1 expores the stability and dependence of boolean functions. Notation \\(\\mathcal P=\\{\\pm 1, \\times\\}\\) for parity, and \\(\\mathcal B=(\\{0, 1\\}, +)\\) for boolean values. \\([n] \\equiv \\{1, \\cdots, n\\}\\), occasionally \\(\\{0, \\cdots, n-1\\}\\) depending on context. \\(\\Delta(x, y)=|\\{i|x_i\\neq y_i\\}|\\) denotes the Hamming distance. \\(A\\Delta B\\) denote the symmetric difference \\((A-A\\cap B)\\cup (B-A\\cap B)\\). Boldface letters like \\(\\mathbf x\\) denote random variables, drawn from a uniform distribution over their domain unless otherwise specified. Under the mapping \\((0, 1)\\) corresponding to false, true, the representation of truth values in \\(\\mathcal P\\) is somewhat counter-intuitively \\((-1, 1)\\mapsto\\) true, false. References O’Donnell, Ryan. 2014. Analysis of Boolean Functions. Cambridge University Press. "],["Sec1.html", "1 Fourier Expansion for General Domain 1.1 Boolean function", " 1 Fourier Expansion for General Domain This section corresponds to chapters \\(1\\) and \\(8\\). We demonstrating how the space of Boolean functions \\(\\mathcal P\\to \\mathbb R\\) form a Hilbert space, and then generalize this construction. 1.1 Boolean function Natural and parity bases Fix \\(n\\) and consider Boolean functions \\(\\mathcal P^n\\to \\mathcal P\\). Without loss of generality we may expand their range to \\(\\mathbb F\\in \\{\\mathbb R, \\mathbb C\\}\\) and define \\[ L^2(\\mathcal P^n) = \\{\\text{all functions $\\mathcal P^n\\to \\mathbb F$}\\} \\] Addition and scalar multiplication are defined in the most obvious way. Definition 1.1 (natural basis of boolean functions) Let \\(I_y\\) where \\(y\\in \\mathcal P^n\\) denote the indicator polynomial \\[ I_y(x) = \\begin{cases} 1 &amp; x = y \\\\ 0 &amp; \\text{else} \\end{cases} \\] The indicator polynomials form the natural basis for boolean functions, since for all \\(f\\in L^2(\\mathcal P^n)\\) \\[ f(x) = f(y)I_y(x) \\implies f = f(y)I_y \\] Note the implicit sum over \\(y\\in \\mathcal P^n\\). Let \\(x_i(y) = y_i, i\\in [n]\\); this is overloading the notation, as \\(x_i\\) can now both be a function or an input variable, depending on context. Definition 1.2 (parity function) Given \\(J\\in \\mathcal P^n\\), define the parity function \\(\\chi_J:\\mathcal P^n\\to \\mathbb F\\). \\[ \\chi_J = \\prod_{J_i=1} x_i \\implies \\chi_J(x) = (-1)^{J\\cdot x} \\] Every indicator function can be expanded in parity functions \\[\\begin{equation} I_y = \\prod_{i=1}^n \\dfrac{1+y_ix_i}{2} \\impliedby (\\exists i: x_i\\neq y_i\\implies I_y=0) \\tag{1.1} \\end{equation}\\] As a consequence, every indicator function \\(I_y\\), thus every boolean function, can be expanded as a multi-linear polynomial in \\((x_j)\\). Note that multilinearity is due to the multilinearity of the indicator expansion (1.1) and the fact that in the \\(\\mathbb F_2\\) representation, \\(x_i^2=1\\). Fourier formulas Theorem 1.1 (fourier expansion of boolean functions) Every boolean function \\(f\\in L^2(\\mathcal P^n)\\) is a unique linear combination of parity functions \\[ f = \\widehat f(J) \\chi_J \\] The coefficients \\(\\widehat f(J)\\) form the fourier spectrum of \\(f\\). Equivalently, \\(\\widehat f\\in L^2(\\mathbb F^n)\\) is the fourier transform of \\(f\\). Note that the range of \\(\\widehat f\\) is in general not \\(\\mathcal P\\). Proof: Note that \\(\\dim L^2(\\mathcal P^n) = 2^n\\). The natural basis \\((I_J)\\) and parity basis \\((\\chi_J)\\) both contain \\(2^n\\) elements and there is a linear transformation between them, so both are bases for \\(L^2(\\mathcal P^n)\\). Remark (product structure of fourier basis). Note that the parity basis function \\(\\chi_J\\) is a product of elements which are supported on (dependent upon) only a component of the input. This is not true of the natural basis. Definition 1.3 (natural and fourier expansions of max) Consider \\(\\max_2\\) with \\(\\max_2(x, y)=-1\\) if \\(x=y=-1\\) else \\(1\\). Using the formula above \\[\\begin{align} {\\max}_2 &amp;= I_{(1, 1)} + I_{(1, -1)} + I_{(-1, 1)} - I_{(-1, -1)} \\\\ &amp;= \\dfrac 1 4 (1+x_1)(1+x_2) + \\dfrac 1 4 (1+x_1)(1-x_2) + \\dfrac 1 4 (1-x_1)(1+x_2) - \\dfrac 1 4 (1-x_1)(1-x_2) \\\\ &amp;= \\cdots = \\dfrac 1 2 (1 + x_1 + x_2 - x_1x_2) \\end{align}\\] Definition 1.4 (inner product) Define the inner product \\(\\langle\\cdot, \\cdot\\rangle: L^2(\\mathcal P^n)\\times L^2(\\mathcal P^n)\\to \\mathbb F\\) via \\[ \\langle f | g\\rangle= \\dfrac 1 {2^n} \\langle f(x)|g(x)\\rangle \\] Again, note the implicit sum over \\(x\\). Here \\(\\langle f(x)|g(x)\\rangle\\) is the inner product on \\(\\mathbb F\\) \\[ \\langle a|b\\rangle= \\begin{cases} ab &amp; \\mathbb F=\\mathbb R\\\\ a^*b &amp; \\mathbb F=\\mathbb C \\end{cases} \\] In most generality \\(\\mathbb F=\\mathbb C\\), in which case \\(\\langle f|g\\rangle= \\mathbb E[f(\\mathbf x)^*g(\\mathbf x)]\\). Norms are defined naturally \\[ \\|f\\|_p = \\mathbb E[|f(\\mathbf x)|^{p}]^{1/p} \\] The indicator basis is apparantly orthonormal since \\(I_xI_y=0\\) if \\(x\\neq y\\). Theorem 1.2 (orthonormality of fourier basis) \\(\\langle\\chi_J|\\chi_K\\rangle= \\delta_{JK}\\). Proof: Using \\(x_j^2=1\\), we obtain \\(\\chi_J\\chi_K=\\chi_{J\\Delta K}\\). We further have \\(\\mathbb E[\\chi_J(\\mathbf x)] = \\delta_{J 0}\\), then \\[ \\langle\\chi_J|\\chi_K\\rangle= \\mathbb E[\\chi_J(\\mathbf x)\\chi_K(\\mathbf x)] = \\mathbb E[\\chi_{J\\Delta K}(\\mathbf x)] = \\delta_{0(J\\Delta K)} = \\delta_{JK} \\] One may also verify that this is non-degenerate, bilinear and (skew)-symmetric. The usual projection and norm formulas follow. Theorem 1.3 (Parseval's theorem) \\(\\|f\\|_2^2 = \\mathbb E[|f(\\mathbf x)|^2] = \\sum_J |\\widehat f(J)|^2\\) In particular, if \\(f\\) is boolean-valued (range in \\(\\mathbb F_2\\)) then \\(\\langle f|f\\rangle= 1\\). Theorem 1.4 (Plancherel's theorem) \\(\\langle f|g\\rangle= \\mathbb E[f(\\mathbf x)^*g(\\mathbf x)] = \\widehat f(J)^*g(J)\\) Definition 1.5 (Hamming distance) Define the Hamming distance between two boolean functions with range \\(\\mathcal P\\) \\[ \\mathrm{dist}(f, g) = \\Pr[f(\\mathbf x)\\neq g(\\mathbf x)] \\] Note that \\(\\langle f|g\\rangle= 1 - 2\\, \\mathrm{dist}(f, g)\\). Definition 1.6 (boolean function statistics) A boolean function, viewed as \\(f(\\mathbf x)\\), is also a real-valued discrete random variable. Its statistics are defined in the most obvious ways \\(\\mathbb E[f] = \\widehat f(0)\\); here we are using \\(\\mathbb F_2\\) and \\(\\mathcal P\\) interchangeably. This is the constant term in the fourier expansion of \\(f\\) about \\(\\mathbb F_2\\) representation. \\(\\mathrm{Cov}(f, g) = \\langle f |g\\rangle- \\mathbb E[f]\\mathbb E[g] = \\sum_{J\\neq 0} \\widehat f(J)^* g(\\mathbf J)\\). Definition 1.7 (fourier weight, spectral sample, degree weight) The (fourier) weight of \\(f\\) on a set \\(J\\) is simply \\(|\\widehat f(J)|^2\\). In particular, Parseval’s theorem implies that the fourier weight of a boolean function defines a probability distribution on the Hamming cube, which may be identified with the power set \\(\\mathcal P([n])\\). The spectral sample \\(\\mathbf S_f\\) is the probability distribution over \\(\\mathcal P([n])\\) according to \\(\\widehat f\\). Each multi-index \\(J\\) has a degree (its polynomial degree in \\((x_j)\\)). The fourier weight of \\(f\\) at degree \\(k\\) is \\[ \\mathbf W^k[f] = \\sum_{|J|=k} |\\widehat f(J)|^2 = \\Pr_{S\\sim \\mathcal S_f}[|S|=k] \\] Relative densities and convolution For this section we use \\(\\mathcal B\\) instead of \\(\\mathcal P\\) to represent the boolean domain, so that addition is the natural operation. Definition 1.8 (density function) A (relative probability) density function on the Hamming cube \\(\\mathcal P^n\\) is a nonnegative function satisfying \\(\\mathbb E[\\varphi(\\mathbf x)] = 1\\). In this case we interchangeably write \\(\\varphi\\) as a density function itself. Given density \\(\\varphi\\), note that \\[ \\mathbb E_{\\mathbf y\\sim \\varphi}[g(\\mathbf y)] = \\dfrac 1 {2^n} \\varphi(y)g(y) = \\langle\\varphi, g\\rangle \\] Definition 1.9 (indicator function) Given \\(A\\subset \\mathcal B^n\\), let \\(1_A:\\mathcal B^n\\to \\mathcal B\\) for the \\((0, 1)\\) indicator function. For \\(A\\neq \\emptyset\\), denote by \\(\\varphi_A = 1_A/\\mathbb E[1_A]\\) the density function associated with uniform distribution over \\(A\\). Example 1.1 (singleton density) Let \\(\\{0\\}\\subset \\mathcal B^n\\) denote the singleton set of the vector with all entries \\(0\\) \\[ \\varphi_{\\{0\\}}(y) = 2^n I_{\\{0\\}}(y) = 2^n \\prod (1+y_j) = \\sum_J \\chi_J(y) \\] Note that \\(\\pm\\) are equivalent in the \\(\\mathcal B\\) algebra. Definition 1.10 (convolution) Given \\(f, g:\\mathcal B^n\\to \\mathbb R\\), define \\(f\\ast g:\\mathcal P^n\\to \\mathbb R\\) by \\[\\begin{equation} (f\\ast g)(x) = \\mathbb E[f(\\mathbf y)g(x\\pm \\mathbf y)] \\tag{1.2} \\end{equation}\\] The counterpart for \\(f, g\\in \\mathcal P^n\\to \\mathbb R\\) is \\((f\\ast g)(x) = \\mathbb E[f(\\mathbf y)g(x\\mathbf y)]\\). Note that if \\(\\varphi\\) is a density function, then \\[ (\\varphi \\ast g)(x) = \\mathbb E_{y\\sim \\varphi}[g(x+\\mathbf y)] \\implies \\mathbb E_{\\mathbf y\\sim \\varphi}[g(\\mathbf y)] = (\\varphi \\ast g)(0) \\] Theorem 1.5 (fourier theorem) Given \\(f, g:\\mathcal B^n\\to \\mathbb C\\), for all \\(J\\) \\[ \\widehat{f\\ast g}(J) = \\widehat f(J) \\widehat g(J) \\] Proof: Using the projection formula, definition, substitute \\(\\mathbf z=\\mathbf x - \\mathbf y\\), and \\(\\chi_J(x+y)=J\\cdot (x+y) = \\chi_J(x)\\chi_J(y)\\) \\[\\begin{align} \\widehat{f\\ast g}(J) &amp;= \\mathbb E_{\\mathbf x}[(f\\ast g)(\\mathbf x) \\chi_J(\\mathbf x)] = \\mathbb E_{\\mathbf{x, y}}[f(\\mathbf y)g(\\mathbf x - \\mathbf y) \\chi_J(\\mathbf x)] \\\\ &amp;= \\mathbb E_{\\mathbf{x, z}}[f(\\mathbf y)g(\\mathbf z)\\chi_J(\\mathbf z + \\mathbf y)] \\\\ &amp;= \\mathbb E_{\\mathbf{x, z}}[f(\\mathbf y)\\chi_J(\\mathbf y) f(\\mathbf z)\\chi_J(\\mathbf z)] = \\widehat f(J)\\widehat g(J) \\end{align}\\] This suffices to prove that convolution is associative and commutative. Almost linear functions, property testing, and BLR Definition 1.11 ((approximate) linearity of boolean functions) A function \\(f:\\mathcal B^n\\to \\mathcal B\\) is linear if either of the following conditions hold: \\(\\forall x, y: f(x+y) = f(x)+f(y)\\) \\(f(x) = a\\cdot x\\) for some \\(a\\in \\mathcal B^n\\) These conditions are indeed equivalent: \\(2\\implies 1\\) is trivial. Given \\(1\\), define \\((e_j)_k=\\delta_{jk}\\), then \\((2)\\) is satisfied by \\(a_j=f(e_j)\\). We propose two definitions of approximately linear: \\(f(x+y)=f(x)+f(y)\\) for almost all pairs \\(x, y\\) For some \\(a\\), \\(f(x)=a\\cdot x\\) for almost all \\(x\\). Here \\(2\\implies 1\\) is robust: if \\((2)\\) is satisfied for \\(x, y\\), then \\(1\\) is satisfied for the pair, too. The converse is not papparant but nevertheless true. Definition 1.12 (closeness) Recalling the Hamming distance 1.5. Two boolean functions are \\(\\epsilon\\)-close of \\(\\mathrm{dist}(f, g)\\leq \\epsilon\\) otherwise \\(\\epsilon\\)-far. Given a nonempty property \\(P\\) on \\(n\\)-bit Boolean functions, define \\[ \\mathrm{dist}(f, P) = \\min_{g\\in P} \\mathrm{dist}(f, g) \\] We define approximate linearity by criterion (2) above: \\(f\\) is \\(\\epsilon\\)-close to being linear if there is a linear function \\(x\\mapsto a\\cdot x\\) which is less than \\(\\epsilon\\)-close to \\(f\\). The test below shows that \\(1\\implies 2\\) above: Theorem 1.6 (BLR test) The BLR procedure tests for linearity as follows: Choose \\(\\mathbf{x, y}\\in \\mathcal B^n\\) independently. Query \\(f\\) at \\(\\mathbf{x, y}, \\mathbf x + \\mathbf y\\). Accept if \\(f(\\mathbf x)+f(\\mathbf y) = f(\\mathbf x + \\mathbf y)\\). If BLR test accepts \\(f\\) with probability \\(1-\\epsilon\\), then \\(f\\) is \\(\\epsilon\\)-close to being linear. Proof: Encode output by \\(\\pm 1\\) so that acceptance condition becomes \\(f(\\mathbf x)f(\\mathbf y) = f(\\mathbf x + \\mathbf y)\\), then \\[ \\dfrac 1 2 + \\dfrac 1 2 f(\\mathbf x)f(\\mathbf y)f(\\mathbf x + \\mathbf y) = \\begin{cases} 1 &amp; f(\\mathbf x)f(\\mathbf y) = f(\\mathbf x + \\mathbf y) \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Note that the input representation is still \\(\\mathcal B\\) so formula (1.2) still applies, then \\[\\begin{align} 1-\\epsilon &amp;= \\Pr(\\text{accept}) = \\dfrac 1 2 + \\dfrac 1 2 \\mathbb E_{\\mathbf x}[f(\\mathbf x) \\mathbb E_{\\mathbf y}[f(\\mathbf y)f(\\mathbf x + \\mathbf y)]] \\\\ &amp;= \\dfrac 1 2 + \\dfrac 1 2 \\mathbb E_{\\mathbf x}[f(\\mathbf x) (f\\ast f)(\\mathbf x)] \\\\ &amp;= \\dfrac 1 2 + \\dfrac 1 2 \\langle f, f\\ast f\\rangle= \\dfrac 1 2 + \\dfrac 1 2 \\sum_J \\hat f(J)^3 \\\\ 1 - 2\\epsilon &amp;= \\sum_J \\hat f(J)^3 \\leq [\\max_K \\hat f(K)] \\sum_J \\hat f(J)^2 = \\max_K \\hat f(K) \\end{align}\\] For the last equation we used \\(\\sum \\hat f(J)^2 = \\langle f, f\\rangle= \\mathbb E[f^2] = 1\\). Now, recall that all linear functions are of the form \\(\\chi_J\\), then \\[ 1 - 2\\epsilon \\max_K \\hat f(K) = \\max_K \\langle f, \\chi_K\\rangle= \\max_K [1 - 2\\mathrm{dist}(f, \\chi_K)] = 1 - 2\\min_K \\mathrm{dist}(f, \\chi_K) \\] It follows that \\(\\epsilon \\geq \\min_K \\mathrm{dist}(f, \\chi_K)\\). If \\(f\\) is \\(\\epsilon\\)-close to \\(\\chi_J\\), we can determine \\(\\chi_J(x)\\) with high probability for a given \\(x\\) using the following procedure: Proposition 1.1 (local correctibility of linear functions) Suppose \\(f:\\mathcal B^n\\to \\mathcal P\\) is \\(\\epsilon\\)-close to \\(\\chi_J\\), then for every \\(x\\in \\mathcal B^n\\), the following procedure outputs \\(\\chi_J(x)\\) with probability at least \\(1-2\\epsilon\\): Choose \\(\\mathbf y\\sim \\mathcal B^n\\) Query \\(f\\) at \\(\\mathbf y\\) and \\(x+\\mathbf y\\) Output \\(f(\\mathbf y)f(x+\\mathbf y)\\) Note that even if \\(f(x)\\neq \\chi_J(x)\\), the procedure above is still able to determine \\(\\chi_J(x)\\) with high probability based on \\(f\\). Proof: The probability that \\(f(x+\\mathbf y)\\neq \\chi_J(x+\\mathbf y)\\) and \\(f(\\mathbf y)\\neq \\chi_J(\\mathbf y)\\) is upper-bounded by \\(1-2\\epsilon\\). "],["influence-and-spectral-learning.html", "2 Influence and Spectral Learning 2.1 Influence, Noise Stablity, and Social Choice", " 2 Influence and Spectral Learning 2.1 Influence, Noise Stablity, and Social Choice This subsection corresponds to Chapter 2. Our focus is on the uniformity and dependence behavior of functions on their inputs. Social choice functions A boolean function \\(\\mathcal P^n\\to \\mathcal P\\) may also be viewed as a voting function for an election with \\(2\\) candidates and \\(n\\) voters. The most common voting function, for odd \\(n\\) is \\[ \\mathrm{Maj}_n(x) = \\mathrm{sgn}\\left(\\sum x_j\\right) \\] Definition 2.1 (dictator, junta) The \\(i\\)-th dictator (projection) function is \\(\\chi_i(x) = x_i\\). Similarly, a function \\(\\mathcal P^n\\to \\mathcal P\\) is a \\(k\\)-junta if it depends upon at most \\(k\\) input coordinates. Informally, a function is a junta if it depends on a constant number of coordinates. Definition 2.2 (weighted majority (linear threshold) function) \\(f\\) is a weighted majority function or, equivalently, a (linear) threshold function if, for some \\(a\\in \\mathbb R^n\\) \\[ f(x) = \\mathrm{sgn}(a\\cdot x) \\] A depth-\\(d\\) recursive majority of \\(n\\) functions, denoted \\(\\mathrm{Maj}_n^{\\otimes d}\\) is a function of \\(n^d\\) bits defined inductively \\[ \\mathrm{Maj}_n^{\\otimes (d+1)}(x^{(1)}, \\cdots, x^{(n)}) = \\mathrm{Maj}_n( \\mathrm{Maj}_n^{\\otimes d}(x^{(1)}), \\cdots, \\mathrm{Maj}_n^{\\otimes d}(x^{(n)}) ) \\] Definition 2.3 (tribes function) The tribes function of width \\(w\\) and size \\(s\\) \\(\\mathrm{Tribes}_{ws}:\\mathcal P^{ws}\\to \\mathcal P\\) is \\[ \\mathrm{Tribes}((x_{ws})) = \\bigcup_{j=1}^s \\left(\\bigcap_{k=1}^w x_{ws}\\right) \\] There are \\(s\\) tribes, each containing \\(w\\) elements. Dependence properties Definition 2.4 (miscellaneous properties) \\(f:\\mathcal P^n\\to \\mathcal P\\) (naturally extending to codomain \\(\\mathbb R\\)) is monotone if \\(x\\leq y\\implies f(x)\\leq f(y)\\) where \\(x\\leq y\\) is defined coordinate-wise. odd if \\(f(-x) = -f(x)\\). unanimous if \\(f(1, \\cdots, 1)=1\\) and \\(f(-1, \\cdots, -1)=-1\\). symmetric if \\(f\\circ \\pi = f\\) for all permutations. In other words, \\(f\\) only depends on the weight of \\(x\\). Theorem 2.1 (May's theorem) \\(f:\\mathcal P^n \\to \\mathcal P\\) is symmetric and monotone if and only if \\(f(x)=\\mathrm{sgn}(1\\cdot x+a_0)\\). If \\(f\\) is additionally odd, then \\(n\\) must be odd and \\(f=\\mathrm{Maj}_n\\). Proof: Symmetry demands \\(f(x)=g(1\\cdot x)\\) for some \\(g:\\mathbb N\\to \\mathbb R\\). One may show that \\(y\\mapsto \\mathrm{sgn}(|y|+a_0)\\) is the only admissible choice. Properties of functions we have seen so far: Majority function for \\(n\\) odd has all four properties. The dictator function is monotone, odd, and unanimous, so do recursive majority functions. Tribes functions are monotone, unanimous, and transitive-symmetric. Definition 2.5 (transitive symmetry) \\(f\\) is transitive-symmetric if for all \\(j, k\\in [n]\\) there exists a permutation \\(\\pi\\in S_n\\) with \\(\\pi(j)=k\\) such that \\(f\\circ \\pi = f\\). Symmetry demands that all arrangements of coordinates are equivalent. Transitive symmetry only demands that every pair of coordinates are equivalent. Definition 2.6 (impartial culture assumption) The impartial culture assumption assumes that all \\(n\\) voters’ preferences are independent and uniformly random. This corresponds to voting in the absence of information. Influence and derivatives Given a coordinate \\(i\\in [n]\\) and binary string \\(x\\), let \\(x^{\\oplus i}\\) denote the same string except with the \\(i\\)-th component flipped. Definition 2.7 (pivot, influence) A coordinate \\(i\\in [n]\\) is pivotal for \\(f\\) on input \\(x\\) if \\(f(x)\\neq f(x^{\\oplus i})\\). The influence of \\(i\\) on \\(f\\) is the probability that \\(i\\) is pivotal for random input: \\[ \\mathrm{Inf}_i[f] = \\Pr[f(\\mathbf x)\\neq f(\\mathbf x^{\\oplus i})] \\] "],["references.html", "References", " References O’Donnell, Ryan. 2014. Analysis of Boolean Functions. Cambridge University Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
